# AI Job Agent ğŸ¤–

An intelligent automation system that scrapes job postings, tailors resumes using AI, and automates job application submissions.

## ğŸ“‹ Overview

AI Job Agent is a FastAPI-based application that leverages LangChain, OpenAI, and Playwright to streamline the job application process. It consists of three core services that work together to automate job hunting:

1. **Job Scraper**: Extracts job details from various platforms (LinkedIn, Indeed, etc.)
2. **Resume Tailoring**: Uses LLM to customize resumes for specific job descriptions
3. **Application Submitter**: Automates form filling and submission (LinkedIn Easy Apply)

## âœ¨ Features

- ğŸ” **Intelligent Job Scraping**: Playwright-based scrapers for dynamic content
- ğŸ¤– **AI-Powered Resume Customization**: LangChain + OpenAI GPT-3.5 integration
- ğŸ“ **Automatic Application Submission**: Browser automation with Playwright
- ğŸ’¾ **Database Persistence**: SQLAlchemy with PostgreSQL/SQLite support
- ğŸš€ **RESTful API**: FastAPI with automatic interactive documentation
- ğŸ›¡ï¸ **Robust Error Handling**: Graceful degradation when services unavailable
- ğŸ§ª **Comprehensive Testing**: Unit and integration tests with pytest

## ğŸ› ï¸ Tech Stack

### Backend
- **FastAPI**: Modern async web framework
- **LangChain**: LLM orchestration framework
- **OpenAI API**: GPT-3.5-turbo for resume tailoring
- **Playwright**: Browser automation for scraping/submission
- **SQLAlchemy**: ORM for database operations

### Database
- **PostgreSQL**: Production database (recommended)
- **SQLite**: Development/testing database

### Libraries
- **BeautifulSoup4**: HTML parsing
- **Pydantic**: Data validation
- **Uvicorn**: ASGI server

## ğŸ“¦ Installation

### Prerequisites
- Python 3.10+
- PostgreSQL (optional, SQLite works for development)
- OpenAI API key

### Setup

1. **Clone the repository**
```bash
git clone <repository-url>
cd ai-job-agent
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Install Playwright browsers**
```bash
playwright install chromium
```

5. **Configure environment variables**
```bash
cp .env.example .env
# Edit .env with your credentials
```

Required environment variables:
```env
DATABASE_URL=postgresql://user:password@localhost/dbname
OPENAI_API_KEY=sk-your-openai-api-key
```

6. **Initialize database**
```bash
# Database tables are created automatically on first run
# Or manually with:
python -c "from app.database import engine, Base; from app.models import *; Base.metadata.create_all(bind=engine)"
```

## ğŸš€ Usage

### Start the server

**Development:**
```bash
uvicorn app.main:app --reload
```

**Production:**
```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

### API Documentation

Once running, visit:
- **Interactive Docs**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### API Endpoints

#### 1. Scrape Job Posting
```bash
POST /jobs/scrape
Content-Type: application/json

{
  "url": "https://www.linkedin.com/jobs/view/123456"
}
```

**Response:**
```json
{
  "id": 1,
  "title": "Senior Software Engineer",
  "company": "Tech Corp",
  "description": "Job description...",
  "url": "https://...",
  "source": "linkedin",
  "created_at": "2025-11-24T12:00:00"
}
```

#### 2. Tailor Resume
```bash
POST /resumes/tailor
Content-Type: application/json

{
  "base_resume": "Your base resume content...",
  "job_description": "Target job description..."
}
```

**Response:**
```json
{
  "id": 1,
  "content": "Tailored resume content optimized for the job...",
  "base_resume": false,
  "created_at": "2025-11-24T12:00:00"
}
```

#### 3. Submit Application
```bash
POST /applications/submit
Content-Type: application/json

{
  "job_id": 1,
  "resume_id": 1
}
```

**Response:**
```json
{
  "id": 1,
  "job_id": 1,
  "resume_id": 1,
  "status": "submitted",
  "created_at": "2025-11-24T12:00:00"
}
```

## ğŸ“ Project Structure

```
ai-job-agent/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # FastAPI application & endpoints
â”‚   â”œâ”€â”€ database.py             # Database configuration
â”‚   â”œâ”€â”€ models.py               # SQLAlchemy models
â”‚   â”œâ”€â”€ agent.py                # LangChain agent setup
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ scraper.py          # Base scraper & factory
â”‚       â”œâ”€â”€ resume.py           # LLM resume builder
â”‚       â”œâ”€â”€ submitter.py        # Application submitter
â”‚       â””â”€â”€ scrapers/
â”‚           â”œâ”€â”€ __init__.py
â”‚           â””â”€â”€ linkedin.py     # LinkedIn-specific scraper
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_basic.py           # Service unit tests
â”‚   â”œâ”€â”€ test_scraper.py         # Scraper tests
â”‚   â”œâ”€â”€ test_resume_llm.py      # LLM integration tests
â”‚   â””â”€â”€ test_api.py             # API endpoint tests
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## ğŸ”„ Implementation Workflow

### 1. Job Scraping Flow
```
User provides URL
    â†“
ScraperFactory selects appropriate scraper
    â†“
Playwright launches browser â†’ Navigates to URL
    â†“
BeautifulSoup parses HTML â†’ Extracts data
    â†“
Job saved to database â†’ Returns Job object
```

### 2. Resume Tailoring Flow
```
User provides base resume + job description
    â†“
ResumeBuilder initializes ChatOpenAI
    â†“
Prompt template formats input
    â†“
LLM generates tailored resume
    â†“
Resume saved to database â†’ Returns Resume object
```

### 3. Application Submission Flow
```
User provides job_id + resume_id
    â†“
Application record created (status: PENDING)
    â†“
ApplicationSubmitter routes to correct submitter
    â†“
LinkedIn: Launch browser â†’ Login â†’ Navigate to job â†’ Click Easy Apply
    â†“
Fill form fields â†’ Submit â†’ Update status
```

## ğŸ§ª Testing

Run all tests:
```bash
DATABASE_URL=sqlite:///./test.db pytest tests/
```

Run specific test file:
```bash
DATABASE_URL=sqlite:///./test.db pytest tests/test_api.py -v
```

With coverage:
```bash
DATABASE_URL=sqlite:///./test.db pytest --cov=app tests/
```

## âš™ï¸ Configuration

### Database Options

**PostgreSQL (Production):**
```env
DATABASE_URL=postgresql://user:password@localhost:5432/jobagent
```

**SQLite (Development):**
```env
DATABASE_URL=sqlite:///./dev.db
```

### Scraper Configuration

The `ScraperFactory` automatically selects scrapers based on URL:
- Contains `linkedin.com` â†’ `LinkedInScraper`
- Default â†’ `MockScraper`

### LLM Configuration

Edit `app/services/resume.py` to customize:
```python
self.llm = ChatOpenAI(
    temperature=0.7,        # Creativity (0-1)
    model="gpt-3.5-turbo"  # Model selection
)
```

Available models:
- `gpt-3.5-turbo` (faster, cheaper)
- `gpt-4` (higher quality, slower)

## ğŸ”’ Security Notes

- Never commit `.env` file or API keys to version control
- Use environment variables for all sensitive data
- Implement rate limiting for production deployments
- Consider using background tasks (Celery/Arq) for long-running operations
- Add authentication/authorization for production API

## ğŸš§ Current Limitations

1. **LinkedIn Scraper**: Only handles public job pages (no authentication)
2. **Application Submitter**: Skeleton implementation (requires manual login setup)
3. **Error Recovery**: Limited retry logic for network failures
4. **Rate Limiting**: No built-in request throttling

## ğŸ¯ Future Enhancements

- [ ] Complete LinkedIn Easy Apply automation with cookie-based auth
- [ ] Add Indeed, Glassdoor scrapers
- [ ] Implement background task queue (Celery)
- [ ] Add job matching/filtering based on criteria
- [ ] Email notifications for application status
- [ ] Web UI dashboard for monitoring
- [ ] Export applications to CSV/PDF
- [ ] Multi-tenant support with user authentication

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see LICENSE file for details.

## ğŸ™ Acknowledgments

- [FastAPI](https://fastapi.tiangolo.com/) - Web framework
- [LangChain](https://python.langchain.com/) - LLM orchestration
- [Playwright](https://playwright.dev/) - Browser automation
- [OpenAI](https://openai.com/) - GPT models

## ğŸ“§ Support

For issues and questions:
- Open an issue on GitHub
- Contact: [your-email@example.com]

---

**Note**: This tool is for educational purposes. Always respect websites' Terms of Service and robots.txt when scraping. Use responsibly and ethically.
